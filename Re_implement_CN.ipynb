{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Re-implement_CN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SebastianJia/nlp_research_conceptor/blob/master/Re_implement_CN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "a7t_W6SsklFo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data\n",
        "  "
      ]
    },
    {
      "metadata": {
        "id": "ph_Lvq6pk6pi",
        "colab_type": "code",
        "outputId": "f981caca-24b1-4283-dcfc-b7b0c50d9862",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy, requests, codecs, os, re, nltk, itertools, csv\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
        "import tensorflow as tf\n",
        "from scipy.stats import spearmanr\n",
        "import pandas as pd\n",
        "import functools as ft\n",
        "import os\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "URfoVDwtlD7N",
        "colab_type": "code",
        "outputId": "d1af283f-1faf-411e-f8e5-3310c5165258",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -q gdown\n",
        "!gdown https://drive.google.com/uc?id=1U_UGB2vyTuTIcbV_oeDtJCtAtlFMvXOM # download a small subset of glove\n",
        "!gdown https://drive.google.com/uc?id=1j_b4TRpL3f0HQ8mV17_CtOXp862YjxxB # download a small subset of word2vec\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1U_UGB2vyTuTIcbV_oeDtJCtAtlFMvXOM\n",
            "To: /content/small_glove.txt\n",
            "333MB [00:04, 82.8MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1j_b4TRpL3f0HQ8mV17_CtOXp862YjxxB\n",
            "To: /content/small_word2vec.txt\n",
            "267MB [00:02, 90.9MB/s]\n",
            "sample_data  small_glove.txt  small_word2vec.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KlY6uIVFpGM7",
        "colab_type": "code",
        "outputId": "3ed3664a-6a9b-4fce-c295-0fe37144abd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        }
      },
      "cell_type": "code",
      "source": [
        "!wget https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki-news-300d-1M.vec.zip\n",
        "!unzip wiki-news-300d-1M.vec.zip\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-01-17 02:21:55--  https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki-news-300d-1M.vec.zip\n",
            "Resolving s3-us-west-1.amazonaws.com (s3-us-west-1.amazonaws.com)... 52.219.24.21\n",
            "Connecting to s3-us-west-1.amazonaws.com (s3-us-west-1.amazonaws.com)|52.219.24.21|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 681808098 (650M) [application/zip]\n",
            "Saving to: ‘wiki-news-300d-1M.vec.zip’\n",
            "\n",
            "wiki-news-300d-1M.v 100%[===================>] 650.22M  21.3MB/s    in 32s     \n",
            "\n",
            "2019-01-17 02:22:28 (20.6 MB/s) - ‘wiki-news-300d-1M.vec.zip’ saved [681808098/681808098]\n",
            "\n",
            "Archive:  wiki-news-300d-1M.vec.zip\n",
            "  inflating: wiki-news-300d-1M.vec   \n",
            "sample_data\t small_word2vec.txt\twiki-news-300d-1M.vec.zip\n",
            "small_glove.txt  wiki-news-300d-1M.vec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "W_gGAHK2Rmt_",
        "colab_type": "code",
        "outputId": "f7d01477-50f3-4064-d1c9-0442feffadbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "!du -h wiki-news-300d-1M.vec"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2G\twiki-news-300d-1M.vec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pvLMJq5YTflf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load Fasttext, small GloVe and small word2vec data"
      ]
    },
    {
      "metadata": {
        "id": "2YnZZtKhvs6Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "fasttext = KeyedVectors.load_word2vec_format('/content/' + 'wiki-news-300d-1M.vec')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UOIQBmZJxPZe",
        "colab_type": "code",
        "outputId": "de0d8809-1c91-4f09-ed0e-52fb97d85a65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "!python -m gensim.scripts.glove2word2vec -i small_glove.txt -o small_glove_w2v.txt\n",
        "!python -m gensim.scripts.glove2word2vec -i small_word2vec.txt -o small_w2v_w2v.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:125: RuntimeWarning: 'gensim.scripts.glove2word2vec' found in sys.modules after import of package 'gensim.scripts', but prior to execution of 'gensim.scripts.glove2word2vec'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "2019-01-17 02:26:37,164 - glove2word2vec - INFO - running /usr/local/lib/python3.6/dist-packages/gensim/scripts/glove2word2vec.py -i small_glove.txt -o small_glove_w2v.txt\n",
            "2019-01-17 02:26:37,425 - glove2word2vec - INFO - converting 128607 vectors from small_glove.txt to small_glove_w2v.txt\n",
            "2019-01-17 02:26:38,625 - glove2word2vec - INFO - Converted model with 128607 vectors and 300 dimensions\n",
            "/usr/lib/python3.6/runpy.py:125: RuntimeWarning: 'gensim.scripts.glove2word2vec' found in sys.modules after import of package 'gensim.scripts', but prior to execution of 'gensim.scripts.glove2word2vec'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "2019-01-17 02:26:40,325 - glove2word2vec - INFO - running /usr/local/lib/python3.6/dist-packages/gensim/scripts/glove2word2vec.py -i small_word2vec.txt -o small_w2v_w2v.txt\n",
            "2019-01-17 02:26:40,539 - glove2word2vec - INFO - converting 76078 vectors from small_word2vec.txt to small_w2v_w2v.txt\n",
            "2019-01-17 02:26:41,451 - glove2word2vec - INFO - Converted model with 76078 vectors and 300 dimensions\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "maoP-rf9ZX29",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "glove = KeyedVectors.load_word2vec_format('/content/' + 'small_glove_w2v.txt')\n",
        "w2v = KeyedVectors.load_word2vec_format('/content/' + 'small_w2v_w2v.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jGxhPVpSwWYJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Post-processing with CN"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "022ddde5-7f6a-4f22-d276-81a0cf0a31ff",
        "id": "YQit52KnT-G7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        }
      },
      "cell_type": "code",
      "source": [
        "import io\n",
        "def cn_mat(pre_cn_f_name, alpha):\n",
        "  pre_cn_data = eval(pre_cn_f_name)\n",
        "  #word_pairs = set(list(cn_data.keys()))\n",
        "  cn_mat = []\n",
        "  for word in pre_cn_data.vocab:\n",
        "    cn_mat.append(pre_cn_data[word])\n",
        "  word_vec = np.array(cn_mat, dtype = float).T\n",
        "  num_word = word_vec.shape[1]\n",
        "  num_vec = word_vec.shape[0]\n",
        "  print(num_word, num_vec)\n",
        "  corr_mat = np.dot(word_vec, word_vec.T)/num_word\n",
        "  #print('got corr_mat')\n",
        "  concept_mat = corr_mat @ np.linalg.inv(corr_mat + alpha ** (-2) * np.eye(num_vec))\n",
        "  #print('got concep_mat')\n",
        "  new_mat = ((np.eye(num_vec)-concept_mat)@word_vec).T\n",
        "  #print('got new_mat')\n",
        "  return new_mat\n",
        "  \n",
        "cn_fasttext_mat = cn_mat('fasttext', alpha = 2)\n",
        "print('CN preprocess done for fasttext data')\n",
        "cn_glove_mat = cn_mat('glove', alpha = 2)\n",
        "print('CN preprocess done for glove data')\n",
        "cn_w2v_mat = cn_mat('w2v', alpha =2)\n",
        "print('CN preprocess done for w2v data')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "999994 300\n",
            "CN preprocess done for fasttext data\n",
            "128607 300\n",
            "CN preprocess done for glove data\n",
            "76078 300\n",
            "CN preprocess done for w2v data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dpmzKwYldGmC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Experiment 1: Word similarity evaluation"
      ]
    },
    {
      "metadata": {
        "id": "VzqvIqhQdQKL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Load word similarity text data"
      ]
    },
    {
      "metadata": {
        "id": "NMVIlRw8dIXP",
        "colab_type": "code",
        "outputId": "cbbc7884-1091-4de5-c585-d122adb42507",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1388
        }
      },
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/wordSimData/EN-MEN-TR-3k.txt\n",
        "!wget https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/wordSimData/EN-MTurk-287.txt\n",
        "!wget https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/wordSimData/EN-RG-65.txt\n",
        "!wget https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/wordSimData/EN-RW-STANFORD.txt\n",
        "!wget https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/wordSimData/EN-SIMLEX-999.txt\n",
        "!wget https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/wordSimData/EN-SimVerb-3500.txt\n",
        "!wget https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/wordSimData/EN-WS-353-ALL.txt\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-01-17 02:37:22--  https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/wordSimData/EN-MEN-TR-3k.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 53593 (52K) [text/plain]\n",
            "Saving to: ‘EN-MEN-TR-3k.txt’\n",
            "\n",
            "\rEN-MEN-TR-3k.txt      0%[                    ]       0  --.-KB/s               \rEN-MEN-TR-3k.txt    100%[===================>]  52.34K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2019-01-17 02:37:23 (2.03 MB/s) - ‘EN-MEN-TR-3k.txt’ saved [53593/53593]\n",
            "\n",
            "--2019-01-17 02:37:24--  https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/wordSimData/EN-MTurk-287.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7218 (7.0K) [text/plain]\n",
            "Saving to: ‘EN-MTurk-287.txt’\n",
            "\n",
            "EN-MTurk-287.txt    100%[===================>]   7.05K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-01-17 02:37:24 (115 MB/s) - ‘EN-MTurk-287.txt’ saved [7218/7218]\n",
            "\n",
            "--2019-01-17 02:37:26--  https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/wordSimData/EN-RG-65.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1210 (1.2K) [text/plain]\n",
            "Saving to: ‘EN-RG-65.txt’\n",
            "\n",
            "EN-RG-65.txt        100%[===================>]   1.18K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-01-17 02:37:26 (253 MB/s) - ‘EN-RG-65.txt’ saved [1210/1210]\n",
            "\n",
            "--2019-01-17 02:37:28--  https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/wordSimData/EN-RW-STANFORD.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49851 (49K) [text/plain]\n",
            "Saving to: ‘EN-RW-STANFORD.txt’\n",
            "\n",
            "EN-RW-STANFORD.txt  100%[===================>]  48.68K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2019-01-17 02:37:28 (1.89 MB/s) - ‘EN-RW-STANFORD.txt’ saved [49851/49851]\n",
            "\n",
            "--2019-01-17 02:37:30--  https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/wordSimData/EN-SIMLEX-999.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18024 (18K) [text/plain]\n",
            "Saving to: ‘EN-SIMLEX-999.txt’\n",
            "\n",
            "EN-SIMLEX-999.txt   100%[===================>]  17.60K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2019-01-17 02:37:30 (1.44 MB/s) - ‘EN-SIMLEX-999.txt’ saved [18024/18024]\n",
            "\n",
            "--2019-01-17 02:37:31--  https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/wordSimData/EN-SimVerb-3500.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 65315 (64K) [text/plain]\n",
            "Saving to: ‘EN-SimVerb-3500.txt’\n",
            "\n",
            "EN-SimVerb-3500.txt 100%[===================>]  63.78K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2019-01-17 02:37:32 (2.47 MB/s) - ‘EN-SimVerb-3500.txt’ saved [65315/65315]\n",
            "\n",
            "--2019-01-17 02:37:33--  https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/wordSimData/EN-WS-353-ALL.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7405 (7.2K) [text/plain]\n",
            "Saving to: ‘EN-WS-353-ALL.txt’\n",
            "\n",
            "EN-WS-353-ALL.txt   100%[===================>]   7.23K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-01-17 02:37:34 (89.6 MB/s) - ‘EN-WS-353-ALL.txt’ saved [7405/7405]\n",
            "\n",
            "EN-MEN-TR-3k.txt    EN-SimVerb-3500.txt  small_w2v_w2v.txt\n",
            "EN-MTurk-287.txt    EN-WS-353-ALL.txt\t small_word2vec.txt\n",
            "EN-RG-65.txt\t    sample_data\t\t wiki-news-300d-1M.vec\n",
            "EN-RW-STANFORD.txt  small_glove.txt\t wiki-news-300d-1M.vec.zip\n",
            "EN-SIMLEX-999.txt   small_glove_w2v.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kj07C41RvYgc",
        "colab_type": "code",
        "outputId": "c2b6cd69-e3ab-4223-fbd2-e7f08b3c1a87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ipOWZhOTddTD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Compare word similarity scores and calculate Spearman Correlation"
      ]
    },
    {
      "metadata": {
        "id": "weaV_H04UV8v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_sim(data_f_name, cn_f_name, cn_mat, alpha):\n",
        "  cn_data = eval(cn_f_name)\n",
        "  #word_pairs = set(list(cn_data.keys()))\n",
        "  fin = io.open(data_f_name, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "  dataset = []\n",
        "  word_vec = []\n",
        "  keys = []\n",
        "  ls_word = list(cn_data.vocab)\n",
        "  for line in fin:\n",
        "    \n",
        "    tokens = line.rstrip().split()\n",
        "    if tokens[0] in cn_data.vocab and tokens[1] in cn_data.vocab:\n",
        "      dataset.append(((tokens[0], tokens[1]), float(tokens[2])))\n",
        "      id1 = ls_word.index(tokens[0])\n",
        "      id2 = ls_word.index(tokens[1])\n",
        "      word_vec.append(cn_mat[id1])\n",
        "      word_vec.append(cn_mat[id2])\n",
        "      keys.append(tokens[0])\n",
        "      keys.append(tokens[1])\n",
        "  dataset.sort(key = lambda score: -score[1]) #sort based on score\n",
        " # print(cn_data['gem'])\n",
        "  cn_dataset = {}\n",
        "  cn_dataset_list = []\n",
        "  \n",
        "  for ((word1, word2), score) in dataset:\n",
        "    #print(word1, word2)\n",
        "    id1 = ls_word.index(word1)\n",
        "    id2 = ls_word.index(word2)\n",
        "    sim_score = 1 - cosine_similarity(cn_mat[id1].reshape(1,-1), cn_mat[id2].reshape(1,-1))\n",
        "    cn_dataset[(word1, word2)] = sim_score\n",
        "    cn_dataset_list.append(((word1, word2),sim_score))\n",
        "  cn_dataset_list.sort(key = lambda score: score[1])\n",
        "  spearman_list1=[]\n",
        "  spearman_list2=[]\n",
        "  for pos_1, (pair, score_1) in enumerate(dataset):\n",
        "    score_2 = cn_dataset[pair]\n",
        "    pos_2 = cn_dataset_list.index((pair, score_2))\n",
        "    spearman_list1.append(pos_1)\n",
        "    spearman_list2.append(pos_2)\n",
        "  rho = spearmanr(spearman_list1, spearman_list2)\n",
        "  return rho[0] \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EsFoNZEFrPPK",
        "colab_type": "code",
        "outputId": "f67369eb-4500-4220-998c-03d45fa57200",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        }
      },
      "cell_type": "code",
      "source": [
        "dataSets = ['EN-RG-65.txt', 'EN-WS-353-ALL.txt', 'EN-RW-STANFORD.txt', 'EN-MEN-TR-3k.txt', 'EN-MTurk-287.txt', 'EN-SIMLEX-999.txt', 'EN-SimVerb-3500.txt']\n",
        "for dataset in dataSets:\n",
        "    dataSetAddress = '/content/'+  dataset\n",
        "    print('evaluating the data set', dataSetAddress)\n",
        "    print('Fasttext ', 'GloVe ', 'w2v ')\n",
        "    print(\"%.4f\" % get_sim(dataSetAddress, 'fasttext',cn_fasttext_mat, alpha =2), \"%.4f\" % get_sim(dataSetAddress, 'glove', cn_glove_mat, alpha =2), \"%.4f\" % get_sim(dataSetAddress, 'w2v', cn_w2v_mat, alpha =2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "evaluating the data set /content/EN-RG-65.txt\n",
            "Fasttext  GloVe  w2v \n",
            "0.8670 0.7913 0.7972\n",
            "evaluating the data set /content/EN-WS-353-ALL.txt\n",
            "Fasttext  GloVe  w2v \n",
            "0.7335 0.7886 0.6926\n",
            "evaluating the data set /content/EN-RW-STANFORD.txt\n",
            "Fasttext  GloVe  w2v \n",
            "0.5369 0.5898 0.5804\n",
            "evaluating the data set /content/EN-MEN-TR-3k.txt\n",
            "Fasttext  GloVe  w2v \n",
            "0.8064 0.8339 0.7869\n",
            "evaluating the data set /content/EN-MTurk-287.txt\n",
            "Fasttext  GloVe  w2v \n",
            "0.7110 0.7116 0.6662\n",
            "evaluating the data set /content/EN-SIMLEX-999.txt\n",
            "Fasttext  GloVe  w2v \n",
            "0.4567 0.4858 0.4684\n",
            "evaluating the data set /content/EN-SimVerb-3500.txt\n",
            "Fasttext  GloVe  w2v \n",
            "0.3654 0.3632 0.3830\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Rq4P_yGNoXoM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# STS Benchmark"
      ]
    },
    {
      "metadata": {
        "id": "CM2WhrKpd2W1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Load STS datasets"
      ]
    },
    {
      "metadata": {
        "id": "cvypn1sHSDhN",
        "colab_type": "code",
        "outputId": "03bea10b-92f9-463b-8929-fc643d887734",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 937
        }
      },
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/stsbenchmark/sts-dev.csv\n",
        "!wget https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/stsbenchmark/sts-mt.csv\n",
        "!wget https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/stsbenchmark/sts-other.csv\n",
        "!wget https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/stsbenchmark/sts-test.csv\n",
        "!wget https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/stsbenchmark/sts-train.csv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-01-17 03:15:12--  https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/stsbenchmark/sts-dev.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 255680 (250K) [text/plain]\n",
            "Saving to: ‘sts-dev.csv’\n",
            "\n",
            "\rsts-dev.csv           0%[                    ]       0  --.-KB/s               \rsts-dev.csv         100%[===================>] 249.69K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2019-01-17 03:15:12 (4.82 MB/s) - ‘sts-dev.csv’ saved [255680/255680]\n",
            "\n",
            "--2019-01-17 03:15:14--  https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/stsbenchmark/sts-mt.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 513141 (501K) [text/plain]\n",
            "Saving to: ‘sts-mt.csv’\n",
            "\n",
            "sts-mt.csv          100%[===================>] 501.11K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2019-01-17 03:15:14 (7.99 MB/s) - ‘sts-mt.csv’ saved [513141/513141]\n",
            "\n",
            "--2019-01-17 03:15:15--  https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/stsbenchmark/sts-other.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 661674 (646K) [text/plain]\n",
            "Saving to: ‘sts-other.csv’\n",
            "\n",
            "sts-other.csv       100%[===================>] 646.17K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2019-01-17 03:15:16 (10.1 MB/s) - ‘sts-other.csv’ saved [661674/661674]\n",
            "\n",
            "--2019-01-17 03:15:18--  https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/stsbenchmark/sts-test.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 282419 (276K) [text/plain]\n",
            "Saving to: ‘sts-test.csv’\n",
            "\n",
            "sts-test.csv        100%[===================>] 275.80K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2019-01-17 03:15:19 (5.27 MB/s) - ‘sts-test.csv’ saved [282419/282419]\n",
            "\n",
            "--2019-01-17 03:15:20--  https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/stsbenchmark/sts-train.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 901177 (880K) [text/plain]\n",
            "Saving to: ‘sts-train.csv’\n",
            "\n",
            "sts-train.csv       100%[===================>] 880.06K  --.-KB/s    in 0.07s   \n",
            "\n",
            "2019-01-17 03:15:21 (11.7 MB/s) - ‘sts-train.csv’ saved [901177/901177]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yBQJhrZ8Tocw",
        "colab_type": "code",
        "outputId": "ed6c7702-69bb-43cb-c093-b581995f9175",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        }
      },
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "EN-MEN-TR-3k.txt     sample_data\t  sts-other.csv\n",
            "EN-MTurk-287.txt     small_glove.txt\t  sts-test.csv\n",
            "EN-RG-65.txt\t     small_glove_w2v.txt  sts-train.csv\n",
            "EN-RW-STANFORD.txt   small_w2v_w2v.txt\t  wiki-news-300d-1M.vec\n",
            "EN-SIMLEX-999.txt    small_word2vec.txt   wiki-news-300d-1M.vec.zip\n",
            "EN-SimVerb-3500.txt  sts-dev.csv\n",
            "EN-WS-353-ALL.txt    sts-mt.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Oaa8E1Yzo0Tk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import io\n",
        "def load_sts_dataset(fname):\n",
        "      fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "    # For a STS dataset, loads the relevant information: the sentences and their human rated similarity score.\n",
        "      sent_pairs = []\n",
        "      for line in fin:\n",
        "          items = line.rstrip().split('\\t')\n",
        "          if len(items) == 7 or len(items) == 9:\n",
        "              sent_pairs.append((re.sub(\"[^0-9]\", \"\", items[2]) + '-' + items[1] , items[5], items[6], float(items[4])))\n",
        "          elif len(items) == 6 or len(items) == 8:\n",
        "              sent_pairs.append((re.sub(\"[^0-9]\", \"\", items[1]) + '-' + items[0] , items[4], items[5], float(items[3])))\n",
        "          else:\n",
        "              print('data format is wrong!!!')\n",
        "      return pd.DataFrame(sent_pairs, columns=[\"year_task\", \"sent_1\", \"sent_2\", \"sim\"])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_all_sts_dataset():\n",
        "    # Loads all of the STS datasets \n",
        "    resourceFile = '/content/'\n",
        "    sts_train = load_sts_dataset(resourceFile + 'sts-train.csv') \n",
        "    sts_dev = load_sts_dataset(resourceFile + \"sts-dev.csv\")\n",
        "    sts_test = load_sts_dataset(resourceFile + \"sts-test.csv\")\n",
        "    sts_other = load_sts_dataset(resourceFile + \"sts-other.csv\")\n",
        "    sts_mt = load_sts_dataset(resourceFile +\"sts-mt.csv\")\n",
        "    \n",
        "    sts_all = pd.concat([sts_train, sts_dev, sts_test, sts_other, sts_mt ])\n",
        "    \n",
        "    return sts_all\n",
        "\n",
        "sts_all = load_all_sts_dataset()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SNWC3KTveCdc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load dataset by year-task"
      ]
    },
    {
      "metadata": {
        "id": "RUvku9s5mqNW",
        "colab_type": "code",
        "outputId": "d7a2d9a1-ce4e-4c59-f35b-e71f4f74b503",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        }
      },
      "cell_type": "code",
      "source": [
        "def load_by_task_year(sts_all):\n",
        "  sts_task_year = {}\n",
        "  for i in sts_all['year_task']:\n",
        "    indices = [index for index, x in enumerate(sts_all['year_task']) if x == i]\n",
        "    sts_task_year[i] = sts_all.iloc[indices]\n",
        "  return sts_task_year\n",
        "sts_year_task = load_by_task_year(sts_all)\n",
        "print(sts_year_task.keys())\n",
        "print(sts_year_task['2012-MSRvid'][0:5])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['2012-MSRvid', '2014-images', '2015-images', '2014-deft-forum', '2012-MSRpar', '2014-deft-news', '2013-headlines', '2014-headlines', '2015-headlines', '2016-headlines', '2017-track5.en-en', '2015-answers-forums', '2016-answer-answer', '2012-surprise.OnWN', '2013-FNWN', '2013-OnWN', '2014-OnWN', '2014-tweet-news', '2015-belief', '2016-plagiarism', '2016-question-question', '2012-SMTeuroparl', '2012-surprise.SMTnews', '2016-postediting'])\n",
            "     year_task                                         sent_1  \\\n",
            "0  2012-MSRvid                         A plane is taking off.   \n",
            "1  2012-MSRvid                A man is playing a large flute.   \n",
            "2  2012-MSRvid  A man is spreading shreded cheese on a pizza.   \n",
            "3  2012-MSRvid                   Three men are playing chess.   \n",
            "4  2012-MSRvid                    A man is playing the cello.   \n",
            "\n",
            "                                              sent_2   sim  \n",
            "0                        An air plane is taking off.  5.00  \n",
            "1                          A man is playing a flute.  3.80  \n",
            "2  A man is spreading shredded cheese on an uncoo...  3.80  \n",
            "3                         Two men are playing chess.  2.60  \n",
            "4                 A man seated is playing the cello.  4.25  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "utseGdZ-eHoa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load dataset by year"
      ]
    },
    {
      "metadata": {
        "id": "ptl5qiXcotl9",
        "colab_type": "code",
        "outputId": "ce0a0269-5912-483d-ef55-74227b3d013a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "cell_type": "code",
      "source": [
        "sts_year = {}\n",
        "def load_by_year(sts_all):\n",
        "  for year in ['2012', '2013', '2014', '2015', '2016', '2017']:\n",
        "    indices = [index for index, x in enumerate(sts_all['year_task'])if year in x]\n",
        "    # store year as dictionary, [year: year-task]\n",
        "    #year_task = sts_all.iloc[indices]\n",
        "    sts_year[year] = sts_all.iloc[indices]\n",
        "  return sts_year\n",
        "sts_year = load_by_year(sts_all)\n",
        "print(len(sts_year.keys()))\n",
        "print(sts_year['2016'][:5])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6\n",
            "           year_task                                             sent_1  \\\n",
            "5552  2016-headlines  Driver backs into stroller with child, drives off   \n",
            "5553  2016-headlines   Spain Princess Testifies in Historic Fraud Probe   \n",
            "5554  2016-headlines  Senate confirms Obama nominee to key appeals c...   \n",
            "5555  2016-headlines  U.N. rights chief presses Egypt on Mursi deten...   \n",
            "5556  2016-headlines  US Senate confirms Janet Yellen as US Federal ...   \n",
            "\n",
            "                                                 sent_2  sim  \n",
            "5552  Driver backs into mom, stroller with child the...  4.0  \n",
            "5553   Spain princess testifies in historic fraud probe  5.0  \n",
            "5554  Senate approves Obama nominee to key appeals c...  5.0  \n",
            "5555   UN Rights Chief Presses Egypt on Morsi Detention  5.0  \n",
            "5556  Senate confirms Janet Yellen as next Federal R...  5.0  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wmhlTYqMaIrB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Preparation for STS Evaluation\n",
        "\n",
        "\n",
        "*   Define Sentence class, which has raw data and tokenized data\n",
        "*   Get similarity scores based on embeddings\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "GO05uTkmaNBB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Sentence:\n",
        "  def __init__(self, sentence):\n",
        "    self.raw = sentence\n",
        "    normalized = sentence.replace(\"‘\", \"'\").replace(\"’\", \"'\")\n",
        "    self.tokens = [token.lower() for token in nltk.word_tokenize(normalized)]\n",
        "\n",
        "def get_sim(sentences1, sentences2, cn_fname, cn_mat):\n",
        "  model = eval(cn_fname)\n",
        "  embeddings = []\n",
        "  ls_word = list(model.vocab)\n",
        "  for sent_1, sent_2 in zip(sentences1, sentences2):\n",
        "    tokens1 = sent_1.tokens\n",
        "    tokens2 = sent_2.tokens\n",
        "    tokens1 = [token for token in tokens1 if token in model.vocab and token.islower()]\n",
        "    tokens2 = [token for token in tokens2 if token in model.vocab and token.islower()]\n",
        "    ids1 = [ls_word.index(token) for token in tokens1 ]\n",
        "    ids2 = [ls_word.index(token) for token in tokens2 ]\n",
        "    embedding1 = np.average([cn_mat[id] for id in ids1], axis = 0)\n",
        "    embedding2 = np.average([cn_mat[id] for id in ids2], axis = 0)\n",
        "    if isinstance(embedding1, float) or isinstance(embedding2, float):\n",
        "      embeddings.append(np.zeros(300))\n",
        "      embeddings.append(np.zeros(300))\n",
        "    else:\n",
        "      embeddings.append(embedding1)\n",
        "      embeddings.append(embedding2)\n",
        "  sim_score = [cosine_similarity(embeddings[id*2].reshape(1, -1), embeddings[id*2+1].reshape(1, -1))[0][0] for id in range(len(embeddings)//2)]\n",
        "  return sim_score\n",
        "        \n",
        "  \n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QS9nTyJd2K91",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WX0g9N1z9Tx4",
        "colab_type": "code",
        "outputId": "fc77613e-66c8-49f3-e592-e16a9117c5cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1823
        }
      },
      "cell_type": "code",
      "source": [
        "model_list = ['glove', 'w2v', 'fasttext']\n",
        "pearson_cors = {}\n",
        "mat = []\n",
        "for year_task in sts_all['year_task'].unique():\n",
        "  for model in model_list:\n",
        "    if model == 'glove':\n",
        "      mat = cn_glove_mat\n",
        "    elif model == 'w2v':\n",
        "      mat = cn_w2v_mat\n",
        "    elif model == 'fasttext':\n",
        "      mat = cn_fasttext_mat\n",
        "        \n",
        "    sentences1=[Sentence(sent1) for sent1 in sts_year_task[year_task]['sent_1']]\n",
        "    sentences2=[Sentence(sent2) for sent2 in sts_year_task[year_task]['sent_2']]\n",
        "    sim = get_sim(sentences1, sentences2, model, mat)\n",
        "    pearson_correlation = round(scipy.stats.pearsonr(sim, sts_year_task[year_task]['sim'])[0] * 100,2)\n",
        "    pearson_cors[(model, year_task)] = pearson_correlation\n",
        "count = 0\n",
        "for (i,j) in pearson_cors.keys():\n",
        "  if count % 3 ==0:\n",
        "    print('')\n",
        "  count +=1\n",
        "  print(i, j, pearson_cors[(i,j)])\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:1128: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:1128: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:1128: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "glove 2012-MSRvid 62.5\n",
            "w2v 2012-MSRvid 75.22\n",
            "fasttext 2012-MSRvid 66.44\n",
            "\n",
            "glove 2014-images 65.81\n",
            "w2v 2014-images 78.24\n",
            "fasttext 2014-images 63.41\n",
            "\n",
            "glove 2015-images 71.43\n",
            "w2v 2015-images 80.48\n",
            "fasttext 2015-images 71.13\n",
            "\n",
            "glove 2014-deft-forum 37.57\n",
            "w2v 2014-deft-forum 42.8\n",
            "fasttext 2014-deft-forum 40.18\n",
            "\n",
            "glove 2012-MSRpar 41.19\n",
            "w2v 2012-MSRpar 40.3\n",
            "fasttext 2012-MSRpar 45.03\n",
            "\n",
            "glove 2014-deft-news 69.08\n",
            "w2v 2014-deft-news 65.57\n",
            "fasttext 2014-deft-news 64.76\n",
            "\n",
            "glove 2013-headlines 67.0\n",
            "w2v 2013-headlines 64.78\n",
            "fasttext 2013-headlines 67.04\n",
            "\n",
            "glove 2014-headlines 61.71\n",
            "w2v 2014-headlines 61.09\n",
            "fasttext 2014-headlines 63.36\n",
            "\n",
            "glove 2015-headlines 69.18\n",
            "w2v 2015-headlines 68.88\n",
            "fasttext 2015-headlines 69.84\n",
            "\n",
            "glove 2016-headlines 67.19\n",
            "w2v 2016-headlines 65.13\n",
            "fasttext 2016-headlines 66.05\n",
            "\n",
            "glove 2017-track5.en-en 65.42\n",
            "w2v 2017-track5.en-en 73.44\n",
            "fasttext 2017-track5.en-en 61.34\n",
            "\n",
            "glove 2015-answers-forums 48.62\n",
            "w2v 2015-answers-forums 53.66\n",
            "fasttext 2015-answers-forums 45.04\n",
            "\n",
            "glove 2016-answer-answer 39.59\n",
            "w2v 2016-answer-answer 42.22\n",
            "fasttext 2016-answer-answer 39.2\n",
            "\n",
            "glove 2012-surprise.OnWN 67.96\n",
            "w2v 2012-surprise.OnWN 70.82\n",
            "fasttext 2012-surprise.OnWN 65.97\n",
            "\n",
            "glove 2013-FNWN 42.07\n",
            "w2v 2013-FNWN 43.99\n",
            "fasttext 2013-FNWN 36.48\n",
            "\n",
            "glove 2013-OnWN 57.45\n",
            "w2v 2013-OnWN 68.76\n",
            "fasttext 2013-OnWN 58.88\n",
            "\n",
            "glove 2014-OnWN 66.43\n",
            "w2v 2014-OnWN 75.08\n",
            "fasttext 2014-OnWN 67.01\n",
            "\n",
            "glove 2014-tweet-news 75.37\n",
            "w2v 2014-tweet-news 74.55\n",
            "fasttext 2014-tweet-news 69.55\n",
            "\n",
            "glove 2015-belief 59.77\n",
            "w2v 2015-belief 61.29\n",
            "fasttext 2015-belief 50.73\n",
            "\n",
            "glove 2016-plagiarism 70.28\n",
            "w2v 2016-plagiarism 73.5\n",
            "fasttext 2016-plagiarism 63.57\n",
            "\n",
            "glove 2016-question-question 60.74\n",
            "w2v 2016-question-question 64.46\n",
            "fasttext 2016-question-question 49.69\n",
            "\n",
            "glove 2012-SMTeuroparl 52.58\n",
            "w2v 2012-SMTeuroparl 35.14\n",
            "fasttext 2012-SMTeuroparl 53.52\n",
            "\n",
            "glove 2012-surprise.SMTnews 47.69\n",
            "w2v 2012-surprise.SMTnews 50.08\n",
            "fasttext 2012-surprise.SMTnews 53.76\n",
            "\n",
            "glove 2016-postediting 68.9\n",
            "w2v 2016-postediting 69.54\n",
            "fasttext 2016-postediting 62.13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IXtXl2A7Dg8M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}